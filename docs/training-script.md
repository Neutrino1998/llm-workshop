# LLM 系统工程培训讲稿

> 预计时长：45-55 分钟
> 目标听众：不熟悉大模型技术的同事
> 核心目标：建立一个统一的认知框架——理解大模型只做一件事，而我们所有的工程工作都是在做同一件事

---

## 开场（3 分钟）

大家好，今天聊大模型系统工程。

你们经常听到 ChatGPT、RAG、Agent 这些词。但我发现一个问题：大部分介绍都是在讲"这个东西能做什么"，很少有人说清楚"它到底是怎么运作的"。

今天我想换个角度。我会先告诉你们**一个核心原理**，然后用 6 个递进的演示来证明：所有看起来很复杂的技术——多轮对话、工具调用、RAG、Agent——**本质上都在做同一件事**。

看完之后，你们再看到任何大模型产品，应该都能猜到背后大概怎么实现的。

**【打开演示页面】**

---

## 核心原理（3 分钟）

在进入演示之前，我要先讲今天最重要的一个认知。理解了这一点，后面所有东西都是它的推论。

### 大模型只做一件事：文字补全

大模型的本质是**自回归文本生成**。说人话就是：

> 给它一段文字，它预测下一个字。然后把预测出来的字加到末尾，再预测下一个字... 如此循环。

就像手机键盘的自动补全——只不过它的"补全"能力远远超过手机输入法。

你给它 `"中国的首都是"`，它大概率会补全出 `"北京"`。
你给它 `"请用 Python 写一个排序函数:\n"`，它会补全出一段代码。
你给它一段英文加 `"\n翻译成中文:\n"`，它会补全出中文翻译。

**它不"理解"什么，它不"思考"什么——它只是在做统计意义上最合理的文字补全。** 只不过当模型参数足够大、训练数据足够多时，这种"补全"涌现出了令人惊讶的智能行为。

### 我们做的一件事：Context Engineering

如果模型只是在补全文字，那我们工程师在做什么？

**我们做的所有事情，都可以归纳为一个词：Context Engineering —— 上下文工程。**

Context（上下文）就是塞给模型的那段输入文本。模型只看得到这段文本，然后基于它来补全。Context 里有什么，模型就知道什么；Context 里没有的，模型就不知道。

所以：

- 想让模型扮演客服？→ 在 Context 里写"你是一个客服"
- 想让模型有多轮对话的"记忆"？→ 在 Context 里塞进之前的对话历史
- 想让模型回答关于公司内部文档的问题？→ 在 Context 里塞进相关的文档段落
- 想让模型能调用搜索引擎？→ 在 Context 里描述工具的定义和调用方法

**我们从来没有改变模型本身——我们只是在构造更好的 Context。**

接下来 6 个演示，每一个都是一种 Context Engineering 技术。我们来看它们分别在 Context 里"加了什么"。

---

## Stage 1：基础调用 —— 最简单的 Context（5 分钟）

### 概念

大模型对外暴露的形式就是一个 HTTP API。你发一个 JSON 请求，它返回一个 JSON 响应。

核心参数只有一个：**`messages` 数组**。这就是我们说的 Context。

```
Context = messages 数组里的所有文本拼接起来
```

### 演示

**【输入：什么是机器学习？】**

大家看请求体：

```json
{
  "model": "qwen-plus",
  "messages": [
    {"role": "user", "content": "什么是机器学习？"}
  ]
}
```

`messages` 数组里只有一条消息。模型拿到这段文字，然后开始"补全"——它看到这是一个问题，统计意义上最合理的补全就是给出回答。

响应里有两个重要信息：
- `content`：模型补全出来的文字
- `usage`：消耗了多少 token（token 约等于字或词片段，中文大约 1 字 ≈ 1.5 token）

### 关键认知

**大模型是无状态的。** 每次调用都是独立的，它不记得上一次你问了什么。这不是 bug，这是它的本质——它只是一个文字补全函数，输入是 Context，输出是补全结果。没有 Context 就没有"记忆"。

> **Context Engineering 视角**：这一步的 Context 只有用户的一句话。最简单的输入 → 最基本的输出。

---

## Stage 2：System Prompt —— 在 Context 里加"人设"（5 分钟）

### 概念

`messages` 数组里的消息有三种角色：
- `system`：系统指令，告诉模型"你是谁、该怎么表现"
- `user`：用户输入
- `assistant`：模型的回复

System Prompt 就是放在 Context 最前面的一段指令文本。模型开始补全之前，先读到这段话，后续所有输出都会受它的影响。

**同一个问题 + 不同的 System Prompt = 完全不同的回答。** 因为 Context 变了，补全方向也变了。

### 演示

**【输入：什么是机器学习？ → 选择"老师"预设】**

请求体变成了两条消息：

```json
{
  "messages": [
    {"role": "system", "content": "你是一个耐心的老师。用简单的类比和生活中的例子来解释..."},
    {"role": "user",   "content": "什么是机器学习？"}
  ]
}
```

回答用了很多比喻，像在给小朋友讲课。

**【切换到"程序员"预设】**

System Prompt 变成了"你是资深 Python 开发者..."。回答里直接上代码了。

**【切换到"创意"预设，有 emoji 和想象力】**

### 关键认知

同一个模型，没有做任何改动——我们只是在 Context 的开头加了一段不同的文字，输出就完全变了。

这就是 prompt engineering（提示词工程）的核心：**你写的 System Prompt 越精确，模型的输出就越符合你的预期。**

> **Context Engineering 视角**：这一步我们在 Context 前面插入了一段"人设指令"。模型的行为变了，但模型本身没变。

---

## Stage 3：多轮对话 —— 在 Context 里塞历史（5 分钟）

### 概念

模型没有记忆，那 ChatGPT 那种连续对话怎么实现的？

答案很直接：**每次调用时，把之前所有的对话历史都塞进 Context。**

模型不"记得"之前说过什么，是我们的代码把历史全部重新传给它，让它"看到"之前说过什么。

### 演示

**【第一轮：输入"我叫小明"】**

messages 数组里有 system + user + assistant。

**【第二轮：输入"我叫什么"】**

看，messages 变成了：

```
system: "你是一个耐心的老师。"
user:      "我叫小明"
assistant: "你好小明..."
user:      "我叫什么"       ← 新的
```

整个对话历史都在里面。模型读到"我叫小明"这句话，自然就能回答"你叫小明"。

**【继续对话两三轮，观察 token 消耗】**

注意 token 消耗一直在增长。因为每次请求都带着完整的历史——Context 越来越长。

**【点击"清空"，重新问"我叫什么"】**

现在模型回答不了了。因为 Context 里没有"我叫小明"这个信息了。

### 关键认知

**多轮对话 ≠ 模型记住了。多轮对话 = 我们每次把历史全部传进去。**

这就解释了为什么：
- 对话太长会变慢变贵——Context 越来越大
- 有些产品会限制对话轮数——因为 Context 有长度上限（context window）
- 实际工程中需要做**上下文管理**——决定保留哪些历史、压缩或丢弃哪些

> **Context Engineering 视角**：这一步我们在 Context 里不断追加对话历史。Context 越长，模型"知道"的越多，但成本也越高。管理 Context 的长度和质量，是工程的核心挑战之一。

---

## Stage 4：工具调用 —— 在 Context 里加"能力描述"（8 分钟）

### 概念

到目前为止，模型只能"说"——它无法获取实时信息、无法查数据库、无法发邮件。它的知识截止在训练时间点。

工具调用（Function Calling）的原理其实很简单：

1. 我们在 Context 里描述"你有哪些工具可以用、每个工具的参数是什么"
2. 模型看到这些描述后，如果它认为需要使用某个工具，就会**补全出一段特定格式的文字**（JSON），告诉我们它想调什么、传什么参数
3. 我们的代码解析这段 JSON，真正去执行工具，然后把执行结果塞回 Context
4. 模型看到执行结果，再补全出最终回答

**关键：模型不执行工具。它只是补全出了"我想调这个工具"的文字。执行是我们的代码做的。**

### 演示

**【输入：帮我搜索一下最新的 AI Agent 发展趋势】**

**【观察四步执行轨迹】**

**① 首次调用（Context 里加了工具定义）**

```json
{
  "messages": [{"role": "user", "content": "帮我搜索最新的 AI Agent 发展趋势"}],
  "tools": [
    {"type": "function", "function": {"name": "web_search", "parameters": {...}}}
  ]
}
```

tools 字段就是告诉模型"你有一个搜索工具可以用"。

**② 模型决策**

模型的补全结果不是普通文字，而是一个 tool_calls 结构：

```json
"tool_calls": [{"function": {"name": "web_search", "arguments": "{\"query\": \"AI Agent 发展趋势\"}"}}]
```

它"说"的是：我要用 web_search，搜"AI Agent 发展趋势"。但它没有真的去搜——它只是补全出了这段 JSON。

**③ 我们的代码执行工具**

后端拿到这个 JSON，调用博查搜索 API，拿到真实的搜索结果。

**④ 二次调用（Context 里加了工具结果）**

把搜索结果塞回 messages，再调模型：

```json
{
  "messages": [
    {"role": "user", "content": "帮我搜索..."},
    {"role": "assistant", "tool_calls": [...]},
    {"role": "tool", "content": "搜索结果: [1] xxx [2] yyy..."}
  ]
}
```

模型看到 Context 里有了搜索结果，就能基于真实数据补全出最终回答。

### 关键认知

**整个过程至少两次 API 调用，每次都是"构造 Context → 模型补全"：**
- 第一次的 Context 里有工具描述 → 模型补全出"工具调用指令"
- 第二次的 Context 里有工具结果 → 模型补全出"基于结果的回答"

模型全程只在做补全。是我们的代码在"编排"整个流程。

> **Context Engineering 视角**：这一步我们在 Context 里加了两种新信息——工具定义和工具执行结果。模型的"调用工具"能力，本质上是一种特殊的文字补全。

---

## Stage 5：RAG —— 在 Context 里注入私有知识（10 分钟）

### 概念

工具调用解决了"实时信息"的问题。但还有一个问题：**私有知识**。

公司内部文档、产品手册、客户数据——这些模型训练时不可能见过。但我们又不能把整个文档库都塞进 Context——太长了，超出 context window，而且大部分内容和当前问题无关。

RAG（检索增强生成）的做法是：**先找到和问题相关的文档片段，只把相关片段塞进 Context。**

这就是为什么叫"检索增强生成"——用检索来增强模型的生成能力。

接下来每一步的中间数据都可以看到。

### 演示

#### 步骤 1：加载文档

**【从 URL 抓取一篇文章，或上传一个文档】**

这是原始文档，可能有几千上万字。我们不能也不需要把它整个塞进 Context。

#### 步骤 2：切分（Chunking）

**【点击"切分"】**

把长文档切成小块（chunk），默认 1000 字，带 200 字重叠。

为什么要切？
- 后面要做"检索"——检索的粒度是 chunk，不是整篇文档
- 只把相关的 chunk 塞进 Context，不浪费 token

**【可以调整 chunk_size 试试效果】**

切大了，每块信息多但可能混入无关内容；切小了，信息碎片化。这里有工程取舍。

#### 步骤 3：向量化（Embedding）

**【点击"向量化"】**

每个 chunk 被另一个模型（Embedding 模型，不是聊天模型）转成一个高维向量——一串 1024 维的浮点数。

核心原理：**语义相近的文字 → 向量空间中距离相近。** "用户登录验证"和"身份认证"的向量会很接近，虽然字面完全不同。

**看可视化里的小柱子，每根是向量的一个维度。不同 chunk 的柱状图长得不一样——这就是它们的"语义指纹"。**

#### 步骤 4：检索

**【输入一个问题，点击"检索"】**

问题也被转成向量，然后计算和每个 chunk 向量的**余弦相似度**，按分数排序，返回 Top 3。

**注意：这一步完全没有调用大模型。** 只是向量数学计算——非常快，也非常便宜。

看相似度分数：0.85、0.72、0.61... 最相关的排在前面。

#### 步骤 5：组装 Prompt 并生成

**【点击"生成"】**

关键一步来了。看组装后的 Prompt：

```
请基于以下参考资料回答用户问题。如果资料中没有相关信息，请如实说明。

【参考资料】
[1] chunk 内容...
[2] chunk 内容...
[3] chunk 内容...

【用户问题】
xxx
```

**这就是 Context Engineering 的核心操作**——我们通过检索，找到了和问题最相关的文档片段，然后把它们"注入"到 Context 中。模型读到这些内容后，就能基于你的私有知识来补全回答了。

### 关键认知

**RAG 的本质：用检索来构造更好的 Context。**

整个流程分两步：
1. **检索（不用大模型）**：文档 → 切分 → 向量化 → 相似度匹配 → 找到相关片段
2. **生成（用大模型）**：把相关片段塞进 Context → 模型基于这些信息补全回答

模型本身没变，我们只是给了它更好的 Context。

> **Context Engineering 视角**：RAG 是目前最重要的 Context Engineering 技术。它解决了一个核心问题——怎么把海量知识中"和当前问题相关的部分"精准地注入 Context。

---

## Stage 6：Agentic RAG —— 动态构造 Context 的循环（8 分钟）

### 概念

刚才的 RAG 是一条固定流水线：检索 → 生成，走一次就结束。

但现实问题往往需要多步推理：
- 检索结果不够 → 换个关键词再搜
- 需要实时信息补充 → 调用搜索引擎
- 需要交叉验证 → 搜索后再抓取原文确认

**Agentic RAG 的核心区别：Agent 在一个循环中，动态决定"下一步往 Context 里加什么信息"。**

每一轮循环：
1. **Think（思考）**：分析当前 Context 里的信息够不够
2. **Act（行动）**：如果不够，决定用什么工具去获取新信息
3. **Observe（观察）**：把新信息加入 Context
4. 回到 1，直到信息充分，输出最终答案

### 演示

**【直接点击"运行 Agent"，默认问题是调研北京和东京房价并给投资建议】**

这个问题故意设计得需要多步——Agent 不可能一次搜索就搞定，它必须分别搜索北京房价、东京房价，可能还要抓取详细页面。看执行轨迹，每一步都能看到 Agent 的"内心活动"：

**🧠 推理与规划**
Agent 分析问题，发现需要分别调研两个城市，制定计划。——这就是第一轮 Think。

**🔧 调用工具：web_search("北京房价")**
Agent 决定先搜北京房价。后端真正执行搜索，返回结果。——新的信息被加入 Context。

**🧠 继续思考**
Agent 判断还需要东京的数据，决定继续搜索。

**🔧 调用工具：web_search("东京房价")**
第二次搜索。——Context 里又多了一批信息。

**🧠 评估信息充分性**
Agent 回顾当前 Context 里已有两个城市的房价数据，判断够不够给出投资建议。

**📊 生成最终回答**
信息充分了，基于完整的 Context 生成对比分析和投资建议。

### 关键认知

**普通 RAG 是固定的 Context 构造流程，Agentic RAG 是动态的 Context 构造循环。**

|  | 普通 RAG | Agentic RAG |
|--|---------|-------------|
| **Context 构造** | 一次性：检索 → 塞入 → 生成 | 多轮迭代：不断获取新信息、丰富 Context |
| **决策方式** | 预定义的固定流程 | Agent 自主判断下一步做什么 |
| **信息来源** | 单一（向量库） | 多种工具（搜索、网页抓取...） |
| **适用场景** | 简单问答 | 复杂分析、多步推理 |

> **Context Engineering 视角**：Agent 的本质是一个 Context 构造循环——不断通过工具获取新信息、注入 Context，直到 Context 里的信息足以支撑一个好的补全结果。

---

## 总结：一个模型，一种工程（3 分钟）

### 统一视角

回顾一下我们看到的 6 个阶段。它们看起来很不同，但用 Context Engineering 的视角来看，都在做同一件事——**往 Context 里加不同的东西**：

```
Stage 1: 基础调用      →  Context = [用户输入]
Stage 2: System Prompt →  Context = [人设指令] + [用户输入]
Stage 3: 多轮对话      →  Context = [人设指令] + [完整对话历史] + [用户输入]
Stage 4: 工具调用      →  Context = [...] + [工具定义] + [工具执行结果]
Stage 5: RAG          →  Context = [...] + [检索到的文档片段]
Stage 6: Agentic RAG  →  Context = 动态构造，循环丰富，直到信息充分
```

**模型始终只做一件事：基于 Context 补全文字。**
**我们始终只做一件事：构造更好的 Context。**

### 三个核心认知

**① 大模型 = 文字补全机**
它不"思考"，它做统计意义上最合理的补全。但当模型足够大时，补全的质量涌现出了类似"思考"的效果。

**② Context 是唯一的信息来源**
模型只看得到 Context 里的内容。Context 里没有的信息，模型就是不知道——无论这个信息多么重要。

**③ 系统工程 = Context Engineering**
所有技术——prompt engineering、多轮对话管理、工具编排、RAG、Agent——核心目标只有一个：在有限的 context window 里，塞进对当前任务最有价值的信息。

### 一句话总结

> 大模型的能力上限由模型决定，但实际表现由 Context 决定。
> 我们的工作就是构造更好的 Context。

有什么问题吗？

---

## Q&A 预备问题

### 1. Token 是什么？怎么计费的？

Token 是模型处理文本的基本单位，大致相当于"字"或"词片段"。中文大约 1 字 ≈ 1-2 token，英文大约 1 词 ≈ 1-1.3 token。

计费方式：按 `输入 token + 输出 token` 收费。输入 token 就是 Context 的长度（messages 里所有文字的 token 总数），输出 token 就是模型补全出来的文字长度。所以 Context 越长，成本越高——这就是为什么 Context Engineering 不仅要考虑质量，还要考虑效率。

### 2. 为什么向量能表示"语义"？

Embedding 模型在训练时读了海量文本，学到了一种映射方式：把文字映射到高维空间中的点。训练目标是让"意思相近的文字"映射到相近的位置。

最后的效果就是："用户登录验证"和"身份认证"虽然字面完全不同，但它们的向量在高维空间里几乎是邻居。这就让"按意思搜索"成为可能——不用精确匹配关键词。

### 3. RAG 和微调（Fine-tuning）有什么区别？

用一个比喻：

- **RAG** = 给模型一份参考资料来做开卷考试。模型本身没变，但它能看到资料。
- **微调** = 让模型提前学习新知识，然后闭卷考试。模型参数被改变了。

| | RAG | 微调 |
|--|------|------|
| 改变模型？ | 不改变 | 改变参数 |
| 知识更新 | 更新文档即可 | 需要重新训练 |
| 成本 | 低 | 高 |
| 适合场景 | 私有知识问答 | 学习特定风格、格式 |

大部分场景 RAG 就够了。微调适合你想让模型改变"说话方式"而不只是"知道什么"的场景。

### 4. Context window 的限制怎么处理？

这是最核心的工程挑战之一。当前主流模型的 context window 从 8K 到 128K token 不等（部分模型支持更长），但 Context 越长，成本越高、推理越慢。

实际工程中的策略：
- **对话历史管理**：只保留最近 N 轮，或对早期对话做摘要压缩
- **RAG 检索精度**：宁可只检索 Top 3 高质量的 chunk，也不要塞 Top 20 低质量的
- **分层处理**：先让模型概括长文档的主旨，再针对具体问题检索细节
- **长文档切分策略**：按语义边界切分（段落、章节），而不是机械地按字数切

核心思路始终一样：**在有限的 context window 里，塞进价值密度最高的信息。**

### 5. 实际产品里还需要考虑什么？

- **向量数据库**：演示用的是内存存储，生产环境需要 Milvus、Qdrant 等专业向量数据库
- **安全**：Prompt 注入防护——恶意用户可能在输入里写"忽略前面所有指令"，需要防御
- **评估**：RAG 的检索准确率、回答质量都需要量化评估体系
- **缓存**：相似的问题可以复用之前的检索结果和回答，降低成本
- **可观测性**：记录每次调用的 Context 内容、token 消耗、延迟，方便排查问题

### 6. 那 ChatGPT 的"记忆"功能是怎么回事？

ChatGPT 的"记忆"本质上也是 Context Engineering。它在对话中提取你的偏好（比如"用户是 Python 开发者"），存入一个长期记忆库。下次对话时，自动把相关记忆注入 System Prompt。

所以它不是模型"记住"了你——而是平台代码帮你在每次对话的 Context 里插入了之前总结的信息。原理和 RAG 如出一辙。