# LLM 系统工程培训讲稿

> 预计时长：45-55 分钟
> 目标听众：不熟悉大模型技术的同事
> 核心目标：建立一个统一的认知框架——理解大模型只做一件事，而我们所有的工程工作都是在做同一件事

---

## 开场（3 分钟）

大家好，今天聊大模型系统工程。

你们经常听到 ChatGPT、RAG、Agent 这些词。但我发现一个问题：大部分介绍都是在讲"这个东西能做什么"，很少有人说清楚"它到底是怎么运作的"。

今天我想换个角度。我会先告诉你们**一个核心原理**，然后用 6 个递进的演示来证明：所有看起来很复杂的技术——多轮对话、工具调用、RAG、Agent——**本质上都在做同一件事**。

看完之后，你们再看到任何大模型产品，应该都能猜到背后大概怎么实现的。

**【打开演示页面】**

---

## 核心原理（3 分钟）

在进入演示之前，我要先讲今天最重要的一个认知。理解了这一点，后面所有东西都是它的推论。

### 大模型只做一件事：文字补全

大模型的本质是**自回归文本生成**。说人话就是：

> 给它一段文字，它预测下一个字。然后把预测出来的字加到末尾，再预测下一个字... 如此循环。

就像手机键盘的自动补全——只不过它的"补全"能力远远超过手机输入法。

你给它 `"中国的首都是"`，它大概率会补全出 `"北京"`。
你给它 `"请用 Python 写一个排序函数:\n"`，它会补全出一段代码。
你给它一段英文加 `"\n翻译成中文:\n"`，它会补全出中文翻译。

**它不"理解"什么，它不"思考"什么——它只是在做统计意义上最合理的文字补全。** 只不过当模型参数足够大、训练数据足够多时，这种"补全"涌现出了令人惊讶的智能行为。

### 我们做的一件事：Context Engineering

如果模型只是在补全文字，那我们工程师在做什么？

**我们做的所有事情，都可以归纳为一个词：Context Engineering —— 上下文工程。**

Context（上下文）就是塞给模型的那段输入文本。模型只看得到这段文本，然后基于它来补全。Context 里有什么，模型就知道什么；Context 里没有的，模型就不知道。

所以：

- 想让模型扮演客服？→ 在 Context 里写"你是一个客服"
- 想让模型有多轮对话的"记忆"？→ 在 Context 里塞进之前的对话历史
- 想让模型回答关于公司内部文档的问题？→ 在 Context 里塞进相关的文档段落
- 想让模型能调用搜索引擎？→ 在 Context 里描述工具的定义和调用方法

**我们从来没有改变模型本身——我们只是在构造更好的 Context。**

接下来 6 个演示，每一个都是一种 Context Engineering 技术。我们来看它们分别在 Context 里"加了什么"。

---

## Stage 1：基础调用 —— 最简单的 Context（5 分钟）

大模型对外暴露的形式就是一个 HTTP API。你发一个 JSON 请求，它返回一个 JSON 响应。核心参数只有一个：**`messages` 数组**——这就是我们说的 Context。

> 此处演示：输入"什么是机器学习？"并发送

大家看请求体：

```json
{
  "model": "qwen-plus",
  "messages": [
    {"role": "user", "content": "什么是机器学习？"}
  ]
}
```

`messages` 数组里只有一条消息。模型拿到这段文字，开始"补全"——它看到这是一个问题，统计意义上最合理的补全就是给出回答。响应里有两个重要信息：`content` 是模型补全出来的文字，`usage` 是消耗了多少 token（中文大约 1 字 ≈ 1.5 token）。

**要记住的话：大模型是无状态的。** 每次调用都是独立的，它不记得上一次你问了什么。没有 Context 就没有"记忆"——这不是 bug，这是本质。

接下来我们看看，在 Context 前面加一段"人设指令"会怎样。

---

## Stage 2：System Prompt —— 在 Context 里加"人设"（5 分钟）

`messages` 数组里的消息有三种角色：`system` 是系统指令，告诉模型"你是谁、该怎么表现"；`user` 是用户输入；`assistant` 是模型的回复。System Prompt 就是放在 Context 最前面的一段指令文本，模型开始补全之前先读到它，后续所有输出都会受它影响。**同一个问题 + 不同的 System Prompt = 完全不同的回答。**

> 此处演示：同一问题切换不同预设，观察输出变化

请求体变成了两条消息：

```json
{
  "messages": [
    {"role": "system", "content": "你是一个耐心的老师。用简单的类比和生活中的例子来解释..."},
    {"role": "user",   "content": "什么是机器学习？"}
  ]
}
```

选"老师"预设，回答用了很多比喻；切到"程序员"预设，回答里直接上代码；再切到"创意"预设，画风又完全变了。同一个模型，同一个问题——我们只是在 Context 开头加了一段不同的文字，输出就完全不同。

**要记住的话：你写的 System Prompt 越精确，模型的输出就越符合预期。** 这就是 prompt engineering 的核心——不是改模型，是改 Context。

人设有了，但模型还是不记得上一轮说了什么。怎么实现连续对话？

---

## Stage 3：多轮对话 —— 在 Context 里塞历史（5 分钟）

模型没有记忆，那 ChatGPT 那种连续对话怎么实现的？答案很直接：**每次调用时，把之前所有的对话历史都塞进 Context。** 模型不是"记得"之前说过什么，是我们的代码把历史全部重新传给它，让它"看到"之前说过什么。

> 此处演示：多轮对话，观察 messages 数组增长和 token 消耗

第一轮输入"我叫小明"，messages 里有 system + user + assistant。第二轮问"我叫什么"，看 messages 变成了：

```
system:    "你是一个耐心的老师。"
user:      "我叫小明"
assistant: "你好小明..."
user:      "我叫什么"       ← 新的
```

整个对话历史都在 Context 里，模型读到"我叫小明"自然能回答。继续聊几轮，token 消耗一直涨——因为每次都带着完整历史。点"清空"再问"我叫什么"，模型答不上来了，因为 Context 里没有这个信息了。

**要记住的话：多轮对话 ≠ 模型记住了，多轮对话 = 我们每次把历史全部传进去。** 对话越长 Context 越大，成本越高——这就是为什么实际工程中要做上下文管理。

到目前为止，模型只能"说"。下一步，我们让它能"做"——调用外部工具。

---

## Stage 4：工具调用 —— 在 Context 里加"能力描述"（8 分钟）

模型的知识截止在训练时间点，它无法获取实时信息、查数据库、发邮件。工具调用（Function Calling）的原理是：我们在 Context 里描述"你有哪些工具可以用"，模型如果认为需要使用，就补全出一段 JSON 告诉我们它想调什么工具、传什么参数。然后我们的代码真正去执行，把结果塞回 Context，模型再基于结果补全最终回答。**关键：模型不执行工具，它只是补全出了"我想调这个工具"的文字。**

> 此处演示：输入搜索请求，观察四步执行轨迹

看整个过程。首次调用，Context 里加了工具定义：

```json
{
  "messages": [
    {"role": "system", "content": "今天的日期是 2025-07-15。"},
    {"role": "user", "content": "帮我搜索最新的 AI Agent 发展趋势"}
  ],
  "tools": [
    {"type": "function", "function": {"name": "web_search", "parameters": {...}}},
    {"type": "function", "function": {"name": "fetch_url", "parameters": {...}}}
  ]
}
```

模型的补全结果不是普通文字，而是一个 tool_calls 结构：

```json
"tool_calls": [{"function": {"name": "web_search", "arguments": "{\"query\": \"AI Agent 发展趋势\"}"}}]
```

后端拿到这个 JSON，调用博查搜索 API 拿到真实结果，然后塞回 messages 再调模型：

```json
{
  "messages": [
    {"role": "user", "content": "帮我搜索..."},
    {"role": "assistant", "tool_calls": [...]},
    {"role": "tool", "content": "搜索结果: [1] xxx [2] yyy..."}
  ]
}
```

模型看到搜索结果，就能基于真实数据补全最终回答。整个过程至少两次 API 调用，每次都是"构造 Context → 模型补全"。

**要记住的话：模型全程只在做补全，是我们的代码在编排整个流程。** 工具调用本质上是一种特殊的文字补全——模型"调用工具"只是补全出了一段 JSON，真正执行的是我们的代码。

工具调用解决了实时信息的问题。但还有一类问题：公司内部的私有知识怎么办？

---

## Stage 5：RAG —— 在 Context 里注入私有知识（10 分钟）

公司内部文档、产品手册、客户数据——这些模型训练时不可能见过。但我们又不能把整个文档库都塞进 Context，太长了而且大部分和当前问题无关。RAG（检索增强生成）的做法是：**先找到和问题相关的文档片段，只把相关片段塞进 Context。** 接下来每一步的中间数据都可以看到。

**加载文档：** 先拿到原始文档，可能有几千上万字，我们不能也不需要把它整个塞进 Context。

> 此处演示：从 URL 抓取文章或上传文档

**切分（Chunking）：** 把长文档切成小块，默认 1000 字带 200 字重叠。检索的粒度是 chunk 而不是整篇文档，这样只把相关的 chunk 塞进 Context，不浪费 token。切大了信息多但可能混入无关内容，切小了信息碎片化——这里有工程取舍。

> 此处演示：切分文档，调整参数观察效果

**向量化（Embedding）：** 每个 chunk 被 Embedding 模型转成一个高维向量——一串浮点数。核心原理是**语义相近的文字在向量空间中距离相近**——"用户登录验证"和"身份认证"的向量会很接近，虽然字面完全不同。看可视化里的小柱子，每根是向量的一个维度，不同 chunk 的柱状图长得不一样，这就是它们的"语义指纹"。

> 此处演示：向量化，观察向量可视化

**检索：** 用户问题也被转成向量，然后和每个 chunk 向量计算余弦相似度，按分数排序返回 Top 3。**这一步完全没有调用大模型**，只是向量数学计算，非常快也非常便宜。

> 此处演示：输入问题并检索，观察相似度分数

**组装 Prompt 并生成：** 关键一步——把检索到的 chunk 和用户问题组装成完整的 Prompt：

```
请基于以下参考资料回答用户问题。如果资料中没有相关信息，请如实说明。

【参考资料】
[1] chunk 内容...
[2] chunk 内容...
[3] chunk 内容...

【用户问题】
xxx
```

我们通过检索找到了最相关的文档片段，"注入"到 Context 中。模型读到这些内容，就能基于私有知识来回答了。

> 此处演示：生成回答，观察组装后的完整 Prompt

**要记住的话：RAG 的本质是用检索来构造更好的 Context。** 检索阶段不用大模型（切分 → 向量化 → 相似度匹配），生成阶段才用大模型（把相关片段塞进 Context → 补全回答）。模型本身没变，我们只是给了它更好的 Context。

刚才的 RAG 是一条固定流水线，走一次就结束。如果检索结果不够好怎么办？接下来看 Agent 怎么动态解决这个问题。

---

## Stage 6：Agentic RAG —— 动态构造 Context 的循环（8 分钟）

普通 RAG 是"检索 → 生成"走一次就结束，但现实问题往往需要多步推理：检索结果不够就换个关键词再搜，需要实时信息就调搜索引擎，需要交叉验证就抓取原文确认。**Agentic RAG 的核心区别：Agent 在一个循环中动态决定"下一步往 Context 里加什么信息"。** 每一轮循环是 Think（信息够不够）→ Act（用什么工具获取新信息）→ Observe（新信息加入 Context）→ 重复，直到信息充分。

> 此处演示：运行 Agent，观察多步推理和工具调用过程

看执行轨迹——注意 Agent 每次运行的路径可能不同：它可能先搜北京再搜东京，也可能反过来，甚至中途决定抓取某个网页获取详细数据。关键不是它走了哪条路，而是它在每一步都在自主判断"Context 里还缺什么信息"，然后选择工具去补齐。每一步都能看到 Agent 的思考过程和 Context 的增长。

|  | 普通 RAG | Agentic RAG |
|--|---------|-------------|
| **Context 构造** | 一次性：检索 → 塞入 → 生成 | 多轮迭代：不断获取新信息、丰富 Context |
| **决策方式** | 预定义的固定流程 | Agent 自主判断下一步做什么 |
| **信息来源** | 单一（向量库） | 多种工具（搜索、网页抓取...） |
| **适用场景** | 简单问答 | 复杂分析、多步推理 |

**要记住的话：普通 RAG 是固定的 Context 构造流程，Agentic RAG 是动态的 Context 构造循环。** Agent 的本质是不断通过工具获取新信息、注入 Context，直到信息足以支撑一个好的回答。

好，6 个阶段全部看完了。我们来回顾一下。

---

## 总结：一个模型，一种工程（3 分钟）

### 统一视角

回顾一下我们看到的 6 个阶段。它们看起来很不同，但用 Context Engineering 的视角来看，都在做同一件事——**往 Context 里加不同的东西**：

```
Stage 1: 基础调用      →  Context = [用户输入]
Stage 2: System Prompt →  Context = [人设指令] + [用户输入]
Stage 3: 多轮对话      →  Context = [人设指令] + [完整对话历史] + [用户输入]
Stage 4: 工具调用      →  Context = [...] + [工具定义] + [工具执行结果]
Stage 5: RAG          →  Context = [...] + [检索到的文档片段]
Stage 6: Agentic RAG  →  Context = 动态构造，循环丰富，直到信息充分
```

**模型始终只做一件事：基于 Context 补全文字。**
**我们始终只做一件事：构造更好的 Context。**

### 三个核心认知

**① 大模型 = 文字补全机**
它不"思考"，它做统计意义上最合理的补全。但当模型足够大时，补全的质量涌现出了类似"思考"的效果。

**② Context 是唯一的信息来源**
模型只看得到 Context 里的内容。Context 里没有的信息，模型就是不知道——无论这个信息多么重要。

**③ 系统工程 = Context Engineering**
所有技术——prompt engineering、多轮对话管理、工具编排、RAG、Agent——核心目标只有一个：在有限的 context window 里，塞进对当前任务最有价值的信息。

### 一句话总结

> 大模型的能力上限由模型决定，但实际表现由 Context 决定。
> 我们的工作就是构造更好的 Context。

有什么问题吗？

---

## Q&A 预备问题

### 1. Token 是什么？怎么计费的？

Token 是模型处理文本的基本单位，中文大约 1 字 ≈ 1-2 token，英文大约 1 词 ≈ 1-1.3 token。计费按输入 token + 输出 token，Context 越长成本越高——所以 Context Engineering 不仅要考虑质量，还要考虑效率。

### 2. 为什么向量能表示"语义"？

Embedding 模型在训练时读了海量文本，学会把"意思相近的文字"映射到高维空间里相近的位置。所以"用户登录验证"和"身份认证"虽然字面不同，向量几乎是邻居——这就让"按意思搜索"成为可能，不需要精确匹配关键词。

### 3. RAG 和微调（Fine-tuning）有什么区别？

RAG 是给模型一份参考资料做开卷考试，模型本身不变；微调是让模型提前学习新知识做闭卷考试，改变了模型参数。大部分场景 RAG 就够了，微调适合需要改变模型"说话方式"的场景。

| | RAG | 微调 |
|--|------|------|
| 改变模型？ | 不改变 | 改变参数 |
| 知识更新 | 更新文档即可 | 需要重新训练 |
| 成本 | 低 | 高 |
| 适合场景 | 私有知识问答 | 学习特定风格、格式 |

### 4. Context window 的限制怎么处理？

当前主流模型的 context window 从 8K 到 128K token 不等，越长成本越高、推理越慢。工程策略包括：只保留最近 N 轮对话、RAG 只取 Top 3 高质量 chunk、按语义边界切分长文档。核心思路：在有限窗口里塞进价值密度最高的信息。

### 5. 实际产品里还需要考虑什么？

生产环境还需要：专业向量数据库（Milvus、Qdrant）替代内存存储、Prompt 注入防护、RAG 质量评估体系、相似问题缓存降低成本、以及完整的可观测性来记录每次调用的 Context 内容和 token 消耗。

### 6. 那 ChatGPT 的"记忆"功能是怎么回事？

本质也是 Context Engineering——从对话中提取你的偏好存入记忆库，下次对话时自动注入 System Prompt。它不是模型"记住"了你，而是平台代码帮你在 Context 里插入了之前总结的信息，原理和 RAG 如出一辙。
